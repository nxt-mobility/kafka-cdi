= Kafka-CDI - A simple CDI extension for Apache Kafka

image:https://img.shields.io/:license-Apache2-blue.svg[License, link=http://www.apache.org/licenses/LICENSE-2.0]

Making it easy to use Apache Kafka's Java client API in a CDI managed environment!

== Project info

This is a modified fork of link:https://github.com/aerogear/kafka-cdi[Aerogear's Kafka-CDI]. This project is not
affiliated with Aerogear. Feel free to use the original version.

Note that this extension is still in an proof of concept state and *not* production ready.
Use without warranty of fitness for any purpose (see license).

=== Motivation

In working with the kafka-cdi extension we found several use cases that did not work out of the box with
the original extension. Lacking time and effort to get the required changes merged into the upstream
version, we decided to create a fork to see whether our contributions would actually be useful.

*Added features:*

* Support for *protobuf* (as key and value)
* *More annotation based configuration* for producers and consumers
* Better integration with CDI (currently Weld only) enabling a *CDI request scope* for each message delivery
* Improved *exception handling* and *retries* of message delivery to improve the extension towards production readiness
* Additional consumer signature to receive a *batch of multiple messages* in a single invocation; the retry mechanism will retry the whole batch in this case
* Integration with link:https://microprofile.io[microprofile.io] *health*

== Getting started

See the library in action link:https://www.youtube.com/watch?v=JEgj3l0TUA4[here]!

The section below gives an overview on how to use the Kafka CDI library!

=== Maven config

In a Maven managed project simply the following to your `pom.xml`:

[source,xml]
----
...
  <dependency>
    <groupId>cloud.nxtmobility</groupId>
    <artifactId>kafka-cdi-extension</artifactId>
    <version>0.1.8</version>
  </dependency>
...
----

=== Injecting a Kafka Producer

The `@Producer` annotation is used to configure and inject an instance of the `SimpleKafkaProducer` class, which is a simple extension of the original `KafkaProducer` class:

[source,java]
----
...
public class MyPublisherService {

  private Logger logger = LoggerFactory.getLogger(MyPublisherService.class);

  @Producer
  SimpleKafkaProducer<Integer, String> producer;

  /**
   * A simple service method, that sends payload over the wire
   */
  public void hello() {
    producer.send("myTopic", "My Message");
  }
}
----

=== Annotating a bean method to declare it as a Kafka Consumer

The `@Consumer` annotation is used to configure and declare an annotated method as a _callback_ for the internal `DelegationKafkaConsumer`, which internally uses the vanilla `KafkaConsumer`:

[source,java]
----
public class MyListenerService {

  private Logger logger = LoggerFactory.getLogger(MyListenerService.class);
  
  /**
   * Simple listener that receives messages from the Kafka broker
    */
  @Consumer(topics = "myTopic", groupId = "myGroupID")
  public void receiver(final String message) {
    logger.info("That's what I got: " + message);
  }
}
----

Receiving the key and the value is also possible:

[source,java]
----
public class MyListenerService {

  private Logger logger = LoggerFactory.getLogger(MyListenerService.class);

  /**
   * Simple listener that receives messages from the Kafka broker
    */
  @Consumer(topics = "myTopic", groupId = "myGroupID")
  public void receiver(final String key, final String value) {
    logger.info("That's what I got: (key: " + key + " , value:" +  value + ")");
  }
}
----

=== Initial `KStream` and `KTable` support for Stream Processing:

With the `@KafkaStream` annotation the libary supports the Kafka Streams API:
[source,java]
----
@KafkaStream(input = "push_messages_metrics", output = "successMessagesPerJob2")
public KTable<String, Long> successMessagesPerJobTransformer(final KStream<String, String> source) {
    final KTable<String, Long> successCountsPerJob = source.filter((key, value) -> value.equals("Success"))
        .groupByKey()
        .count("successMessagesPerJob");
    return successCountsPerJob;
    }
----

The method accepts a `KStream` instance from the `input` topic, inside the method body some (simple) stream processing can be done, and as return value we support either `KStream` or `KTable`. The entire setup is handled by the library itself.


=== Global Configuration of the Kafka cluster

A minimal of configuration is currently needed. For that there is a `@KafkaConfig` annotation. The first occurrence is used:

[source,java]
----
@KafkaConfig(bootstrapServers = "#{KAFKA_SERVICE_HOST}:#{KAFKA_SERVICE_PORT}")
public class MyService {
   ...
}
----

=== JsonObject Serialization

Apache Kafka uses a binary message format, and comes with a handful of handy Serializers and Deserializers, available through the `Serdes` class. The Kafka CDI extension adds a Serde for the `JsonObject`:

==== JsonObject Serializer

To send serialize a JsonObject, simply specify the type, like:

[source,java]
----
...
@Producer
SimpleKafkaProducer<Integer, JsonObject> producer;
...

producer.send("myTopic", myJsonObj);
----

==== JsonObject Deserializer

For deserialization the argument on the annotation `@Consumer` method is used to setup the actual `Deserializer`

[source,java]
----
@Consumer(topic = "myTopic", groupId = "myGroupID", keyType = Integer.class)
public void receiveJsonObject(JsonObject message) {
  logger.info("That's what I got: " + message);
}
----

== Running Apache Kafka 

To setup Apache Kafka there are different ways to get started. This section quickly discusses pure Docker and Openshift.

=== Running via Docker images

Starting a Zookeeper cluster:

[source,bash]
----
docker run -d --name zookeeper jplock/zookeeper:3.4.6
----

Next, we need to start Kafka and link the Zookeeper Linux container to it:

[source,bash]
----
docker run -d --name kafka --link zookeeper:zookeeper ches/kafka
----

Now, that the broker is running, we need to figure out the IP address of it:

[source,bash]
----
docker inspect --format '{{ .NetworkSettings.IPAddress }}' kafka  
----

We use this IP address when inside our `@KafkaConfig` annotation that our _Producers_ and _Consumers_ can speak to Apache Kafka.

=== Running on Openshift 

For Apache Kafka on Openshift please check this repository:

https://github.com/strimzi/strimzi

